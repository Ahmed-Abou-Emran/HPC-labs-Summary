#include <mpi.h>
#include <iostream>

int main(int argc, char** argv) {
    MPI_Init(&argc, &argv);

    int world_rank, world_size;
    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);
    MPI_Comm_size(MPI_COMM_WORLD, &world_size);

    int message_size = 1;
    int message_buffer[message_size];

    int receiver_rank = (world_rank + 1) % world_size;
    int sender_rank = (world_rank + world_size - 1) % world_size;

    int num_iterations = 10;
    for (int i = 0; i < num_iterations; i++) {
        // Send message to next process
        MPI_Send(message_buffer, message_size, MPI_INT, receiver_rank, 0, MPI_COMM_WORLD);
        std::cout << "Process " << world_rank << " sent message to process " << receiver_rank << std::endl;

        // Receive message from previous process
        MPI_Recv(message_buffer, message_size, MPI_INT, sender_rank, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
        std::cout << "Process " << world_rank << " received message from process " << sender_rank << std::endl;

        receiver_rank = (receiver_rank + 1) % world_size;
        sender_rank = (sender_rank + world_size - 1) % world_size;
    }

    MPI_Finalize();
    return 0;
}

/******** Explanation ********/

// MPI (Message Passing Interface) is a standardized library specification for message passing parallel computing. It allows multiple processes to communicate and synchronize with each other on a distributed memory system.

// #include <mpi.h> and <iostream> are header files that provide necessary functions and definitions for MPI and standard I/O in C++.

// MPI_Init(&argc, &argv) initializes the MPI environment with command line arguments argc and argv.

// MPI_Comm_rank(MPI_COMM_WORLD, &world_rank) retrieves the rank of the current process in the MPI communicator, which is MPI_COMM_WORLD in this case.

// MPI_Comm_size(MPI_COMM_WORLD, &world_size) retrieves the total number of processes in the current MPI communicator.

// The message buffer and message size are defined using C++ syntax.

// The rank of the previous and next processes are calculated using the modulo operator.

// The number of iterations for the ring communication pattern is set to 10, and the loop iterates through the iterations.

// MPI_Send(message_buffer, message_size, MPI_INT, receiver_rank, 0, MPI_COMM_WORLD) sends the message to the next process using MPI_Send.

// std::cout << "Process " << world_rank << " sent message to process " << receiver_rank << std::endl; prints a message indicating that the current process sent a message to the next process.

// MPI_Recv(message_buffer, message_size, MPI_INT, sender_rank, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE) receives the message from the previous process using MPI_Recv.

// std::cout << "Process " << world_rank << " received message from process " << sender_rank << std::endl; prints a message indicating that the current process received a message from the previous process.

// The ranks of the previous and next processes are updated using the modulo operator.

// MPI_Finalize() finalizes the MPI environment before exiting the program.

/******** Summary ********/

// This code initializes the MPI environment, retrieves the rank and size of the current process, defines a message buffer and message size for the point-to-point communication, calculates the ranks of the previous and next processes using the modulo operator, sends and receives messages in a ring pattern using MPI_Send and MPI_Recv, updates the ranks of the previous and next processes, and prints messages indicating the sending and receiving of messages. This example demonstrates the use of MPI point-to-point communication functions in C++ for a ring communication pattern.